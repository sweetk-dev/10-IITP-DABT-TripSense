# streamlit run app.py
# -*- coding: utf-8 -*-
import os
import json
import re
import glob
from io import StringIO
from datetime import datetime
from typing import List, Dict, Any, Optional

import pandas as pd
import streamlit as st

# ======================= ê¸°ë³¸ ì„¤ì • =======================
st.set_page_config(page_title="ë¬´ì¥ì•  ì •ë³´ ì¶”ì²œ", page_icon="ğŸ§­", layout="wide")

# ì›¹ ê²€ìƒ‰/í‰ê°€ì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„° (CSV/JSON ëª¨ë‘ ìë™ ë¡œë“œ)
DATA_PATH = "./data/visitkorea"

RESULT_ROOT = "results"
RUN_DIR = os.path.join(RESULT_ROOT, "runs")
EVAL_DIR = os.path.join(RESULT_ROOT, "eval")
TOPK_FOR_RUN = 5            # run íŒŒì¼ì— ì €ì¥í•  top-k (AP@5 ê¸°ì¤€)
K_FOR_MAP = 5               # eval.py --k 5

PLACE_MAP_KO2CODE = {"ê´€ê´‘ì§€": "ms", "ìˆ™ë°•": "acm", "ìŒì‹ì ": "food"}
DATE_PAT = re.compile(r"^\d{4}\.\d{2}\.\d{2}$")
EPOCH = datetime(1970, 1, 1)

# ======================= ê³µí†µ Helper í•¨ìˆ˜ =======================
def _infer_code_from_url_or_name(rec: Dict[str, Any], name_hint: str = "") -> Optional[str]:
    """URL ë˜ëŠ” íŒŒì¼ëª…ì—ì„œ ms/acm/food ì½”ë“œë¥¼ ì¶”ë¡ """
    url = str(rec.get("detail_url") or "").lower()
    for code in ["ms", "acm", "food"]:
        if f"/{code}/" in url or code in name_hint.lower():
            return code
    return None

def _parse_date(s: Optional[str]) -> Optional[datetime]:
    if not s:
        return None
    s = s.strip()
    if DATE_PAT.match(s):
        try:
            return datetime.strptime(s, "%Y.%m.%d")
        except Exception:
            return None
    return None

def _safe_ts(dt: Optional[datetime]) -> float:
    if not dt:
        return 0.0
    if dt < EPOCH:
        dt = EPOCH
    try:
        return dt.timestamp()
    except Exception:
        return 0.0

def ensure_dir(path: str):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

# ======================= ë°ì´í„° ë¡œë“œ =======================
def load_from_path(data_dir: str) -> List[Dict[str, Any]]:
    """í´ë” ë‚´ CSV/JSON íŒŒì¼ ìë™ ë¡œë“œ (ì›¹ ê²€ìƒ‰ + í‰ê°€ ê³µìš©)"""
    all_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)
                 if f.lower().endswith((".csv", ".json"))]
    if not all_files:
        st.error(f"'{data_dir}' ê²½ë¡œì— CSV/JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    records = []
    for path in all_files:
        name = os.path.basename(path)
        try:
            if path.lower().endswith(".json"):
                with open(path, "r", encoding="utf-8") as f:
                    data = json.load(f)
            else:
                df = pd.read_csv(path, dtype=str, keep_default_na=False)
                data = df.to_dict(orient="records")
            for r in data:
                if "_code" not in r or not r["_code"]:
                    r["_code"] = _infer_code_from_url_or_name(r, name_hint=name)
            records.extend(data)
        except Exception:
            print(f"[WARN] {name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
    return records

# ======================= ì§€ì—­ ê³„ì¸µ ì¶”ì¶œ =======================
def extract_region_hierarchy(records: List[Dict[str, Any]]):
    """area í•„ë“œì—ì„œ ì‹œë„ / ì‹œêµ°êµ¬ ëª©ë¡ ì¶”ì¶œ"""
    areas = [r.get("area", "") for r in records if r.get("area")]
    sido_list, sigungu_map = set(), {}
    for a in areas:
        parts = re.split(r"\s+", a.strip())
        if len(parts) >= 1:
            sido = parts[0]
            sido_list.add(sido)
            sigungu = parts[1] if len(parts) >= 2 else None
            if sido not in sigungu_map:
                sigungu_map[sido] = set()
            if sigungu:
                sigungu_map[sido].add(sigungu)
    return sorted(list(sido_list)), {k: sorted(list(v)) for k, v in sigungu_map.items()}

# ======================= í•„í„°ë§ & ì •ë ¬ (ì›¹ ê²€ìƒ‰ìš©) =======================
def record_supports(rec, targets, mode="all"):
    bf = rec.get("barrierfree_info") or {}
    if not isinstance(bf, dict):
        return False
    if mode == "any":
        return any(t in bf and bf[t] for t in targets)
    return all(t in bf and bf[t] for t in targets)

def matched_list(rec, targets):
    bf = rec.get("barrierfree_info") or {}
    if not isinstance(bf, dict):
        return []
    return [t for t in targets if t in bf and bf[t]]

def filter_and_rank(records, place_code, disabilities, mode="all", sido=None, sigungu=None):
    """
    recommend_poi.py ì™€ ë™ì¼í•œ ì •ë ¬ ê¸°ì¤€ì„ ì‚¬ìš©:
    1) ë§¤ì¹­ ê°œìˆ˜(_count) ë‚´ë¦¼ì°¨ìˆœ
    2) ì œëª©(title) ì˜¤ë¦„ì°¨ìˆœ
    """
    # place ì½”ë“œ í•„í„°
    recs = [r for r in records if r.get("_code") == place_code]

    # ì§€ì—­ í•„í„°
    if sido:
        recs = [r for r in recs if (r.get("area") or "").startswith(sido)]
    if sigungu:
        recs = [r for r in recs if sigungu in (r.get("area") or "")]

    # ì¥ì• ìœ í˜• í•„í„°
    recs = [r for r in recs if record_supports(r, disabilities, mode)]

    # ë§¤ì¹­ ê°œìˆ˜(_count) ê³„ì‚°
    ranked = []
    for r in recs:
        mlist = matched_list(r, disabilities)
        ranked.append({**r, "_matched_list": mlist, "_count": len(mlist)})

    # âœ… recommend_poi.py ì™€ ë™ì¼í•œ ì •ë ¬: matched_count â†“, title â†‘
    ranked.sort(
        key=lambda x: (-x["_count"], (x.get("title") or ""))
    )
    return ranked

# ======================= ë³€í™˜ / ë‹¤ìš´ë¡œë“œ =======================
def to_dataframe(rows):
    return pd.DataFrame([{
        "cotId": r.get("cotId"),
        "title": r.get("title"),
        "area": r.get("area"),
        "update_date": r.get("update_date"),
        "matched_disabilities": ",".join(r.get("_matched_list", [])),
        "detail_url": r.get("detail_url"),
        "lat": r.get("lat"),
        "lng": r.get("lng"),
    } for r in rows])

def df_to_csv_bytes(df):
    buf = StringIO()
    df.to_csv(buf, index=False)
    return buf.getvalue().encode("utf-8-sig")

def rows_to_json_bytes(rows):
    cleaned = [{k: v for k, v in r.items() if not k.startswith("_")} for r in rows]
    return json.dumps(cleaned, ensure_ascii=False, indent=2).encode("utf-8")

# ======================= ë„¤ë¹„ê²Œì´ì…˜ ìƒíƒœ =======================
def go_detail(cotid):
    st.session_state["view"] = "detail"
    st.session_state["selected_cotid"] = cotid

def go_main():
    st.session_state["view"] = "main"
    st.session_state["selected_cotid"] = None

if "view" not in st.session_state:
    st.session_state["view"] = "main"
if "selected_cotid" not in st.session_state:
    st.session_state["selected_cotid"] = None

# ======================= evalìš© AP/MAP ê³„ì‚° í•¨ìˆ˜ =======================
def load_run_files(run_dir: str):
    paths = sorted(glob.glob(os.path.join(run_dir, "run_*.csv")))
    runs = []
    for p in paths:
        df = pd.read_csv(p, dtype={"is_relevant": int}, keep_default_na=False)
        fname = os.path.basename(p)
        query_id = fname.replace("run_", "").replace(".csv", "")
        runs.append((query_id, df))
    return runs

def compute_ap(df: pd.DataFrame, k: Optional[int] = None) -> float:
    df = df.sort_values("rank")
    if k is not None:
        df = df.head(k)
    rels = df["is_relevant"].astype(int).tolist()
    if not rels:
        return 0.0

    hits = 0
    precision_sum = 0.0
    for idx, rel in enumerate(rels, start=1):
        if rel == 1:
            hits += 1
            precision_sum += hits / idx

    if hits == 0:
        return 0.0
    return precision_sum / hits

def recompute_and_save_map(run_dir: str, eval_dir: str, k: Optional[int] = None):
    ensure_dir(eval_dir)
    runs = load_run_files(run_dir)
    if not runs:
        return None, None

    col_name = f"AP@{k or 'all'}"
    ap_rows = []
    for query_id, df in runs:
        ap = compute_ap(df, k=k)
        ap_rows.append({"query_id": query_id, col_name: ap})

    ap_df = pd.DataFrame(ap_rows)
    map_value = ap_df[col_name].mean()

    out_name = f"ap_report@{k}.csv" if k is not None else "ap_report.csv"
    out_path = os.path.join(eval_dir, out_name)
    ap_df.to_csv(out_path, index=False, encoding="utf-8-sig")
    return map_value, out_path

# ======================= í˜„ì¬ í™”ë©´ ê²°ê³¼ â†’ run í¬ë§·ìœ¼ë¡œ ë³€í™˜ =======================
def build_run_df_from_results(
    results: List[Dict[str, Any]],
    place_code: str,
    eval_topk: int,
) -> pd.DataFrame:
    """
    í˜„ì¬ í™”ë©´ì— ë³´ì´ëŠ” ì •ë ¬ëœ results ë¦¬ìŠ¤íŠ¸ì—ì„œ
    ìƒìœ„ eval_topkê°œë¥¼ ë½‘ì•„ recommend_poi.pyì™€ ê°™ì€ í¬ë§·ìœ¼ë¡œ ë³€í™˜.
    (ì •ë ¬/í•„í„°ëŠ” filter_and_rankì™€ 100% ë™ì¼)
    """
    selected = results[:eval_topk]

    rows = []
    for r in selected:
        bf = r.get("barrierfree_info") or {}
        if not isinstance(bf, dict):
            bf = {}
        disability_type = list(bf.keys())

        rows.append({
            "place": place_code,
            "cotId": r.get("cotId"),
            "title": r.get("title"),
            "area": r.get("area"),
            "disability_type": disability_type,
            "is_relevant": 1,  # í™”ë©´ì— ì´ë¯¸ ì¥ì• ìœ í˜•ìœ¼ë¡œ í•„í„°ëœ ê²°ê³¼ì´ë¯€ë¡œ 1ë¡œ ì²˜ë¦¬
        })

    if not rows:
        return pd.DataFrame(columns=[
            "rank", "place", "cotId", "title", "area",
            "disability_type", "is_relevant", "precision"
        ])

    df = pd.DataFrame(rows)
    df["rank"] = range(1, len(df) + 1)
    # ëª¨ë“  ë¬¸ì„œë¥¼ relevantë¡œ ë³´ê³  ìˆìœ¼ë¯€ë¡œ cum_relevant = rank
    df["precision"] = df["rank"].astype(str) + "/" + df["rank"].astype(str)

    # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ recommend_poi.pyì™€ ë™ì¼
    df = df[[
        "rank", "place", "cotId", "title", "area",
        "disability_type", "is_relevant", "precision"
    ]]
    return df

# ======================= ì‹¤ì œ ë°ì´í„° ë¡œë“œ =======================
ALL_RECORDS = load_from_path(DATA_PATH)
SIDO_LIST, SIGUNGU_MAP = extract_region_hierarchy(ALL_RECORDS)

# ======================= Main View =======================
def render_main():
    st.title("ğŸ§­ ë¬´ì¥ì•  ì •ë³´ ì¶”ì²œ")

    # ----- ì‚¬ì´ë“œë°”: ì¡°ê±´ ì…ë ¥ -----
    with st.sidebar:
        st.header("ìƒì„¸ ê²€ìƒ‰")
        place = st.selectbox("ê´€ê´‘ì •ë³´", ["ê´€ê´‘ì§€", "ìˆ™ë°•", "ìŒì‹ì "], index=0)
        sido = st.selectbox("ì‹œë„ ì„ íƒ", ["ì „ì²´"] + SIDO_LIST, index=0)
        sigungu = st.selectbox(
            "ì‹œêµ°êµ¬ ì„ íƒ",
            ["ì „ì²´"] + (SIGUNGU_MAP.get(sido, []) if sido != "ì „ì²´" else []),
            index=0
        )
        dis_sel = st.multiselect("ì¥ì• ìœ í˜•", ["ì§€ì²´ì¥ì• ", "ì‹œê°ì¥ì• ", "ì²­ê°ì¥ì• "], default=["ì§€ì²´ì¥ì• "])
        mode = st.radio("ë§¤ì¹­ ë°©ì‹", ["all", "any"], index=0)
        topk = st.number_input("í‘œì‹œ ê°œìˆ˜", min_value=1, max_value=10000, value=50, step=1)
        map_on = st.checkbox("ì§€ë„ ë³´ê¸°", value=False)

        st.markdown("---")
        st.subheader("í‰ê°€ìš© runs / eval ì €ì¥")

        query_id_input = st.text_input("query_id (ì˜ˆ: Q1, Q2 ë“±)", value="Q1")
        save_clicked = st.button("í˜„ì¬ í™”ë©´ ê²°ê³¼ë¥¼ run / evalë¡œ ì €ì¥")

    # ----- ë©”ì¸ ê²°ê³¼(í™”ë©´ í‘œì‹œìš©) -----
    place_code = PLACE_MAP_KO2CODE[place]
    sido_filter = None if sido == "ì „ì²´" else sido
    sigungu_filter = None if sigungu == "ì „ì²´" else sigungu

    results = filter_and_rank(
        ALL_RECORDS,
        place_code,
        dis_sel,
        mode=mode,
        sido=sido_filter,
        sigungu=sigungu_filter,
    )

    st.write(
        f"**ì¡°ê±´**: ì¥ì†Œ=`{place}`, ì§€ì—­=`{sido}` {sigungu if sigungu!='ì „ì²´' else ''}, "
        f"ì¥ì• ìœ í˜•=`{', '.join(dis_sel)}` / ê²°ê³¼={len(results)}"
    )

    top_rows = results[:topk]
    df = to_dataframe(top_rows)

    st.subheader("ê²€ìƒ‰ ê²°ê³¼")
    for _, row in df.iterrows():
        with st.container(border=True):
            cols = st.columns([4, 3, 2, 2, 2])
            cols[0].markdown(f"**{row['title']}**")
            cols[1].markdown(row["area"] or "")
            cols[2].markdown(row["update_date"] or "")
            cols[3].markdown(row["matched_disabilities"] or "")
            if cols[4].button("ğŸ” ìƒì„¸ ë³´ê¸°", key=f"detail_{row['cotId']}"):
                go_detail(row["cotId"])
                st.rerun()

    st.markdown("### ë‹¤ìš´ë¡œë“œ")
    col1, col2 = st.columns(2)
    with col1:
        st.download_button("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ", df_to_csv_bytes(df), "poi_results.csv", "text/csv")
    with col2:
        st.download_button("â¬‡ï¸ JSON ë‹¤ìš´ë¡œë“œ", rows_to_json_bytes(top_rows), "poi_results.json", "application/json")

    if map_on and {"lat", "lng"}.issubset(df.columns):
        map_df = df.dropna(subset=["lat", "lng"]).rename(columns={"lat": "latitude", "lng": "longitude"})
        if not map_df.empty:
            st.map(map_df[["latitude", "longitude"]])
        else:
            st.info("í‘œì‹œ ê°€ëŠ¥í•œ ìœ„ê²½ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ----- ì €ì¥ ë²„íŠ¼ ë™ì‘ (í™”ë©´ ê²°ê³¼ ê¸°ë°˜) -----
    if save_clicked:
        if not query_id_input.strip():
            st.error("query_idë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            ensure_dir(RUN_DIR)
            # í™”ë©´ì— ë³´ì´ëŠ” ì •ë ¬/í•„í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ run í¬ë§·ìœ¼ë¡œ ë³€í™˜
            df_run = build_run_df_from_results(
                results,
                place_code=place_code,
                eval_topk=TOPK_FOR_RUN,
            )

            run_path = os.path.join(RUN_DIR, f"run_{query_id_input.strip()}.csv")
            df_run.to_csv(run_path, index=False, encoding="utf-8-sig")

            map_value, ap_report_path = recompute_and_save_map(
                RUN_DIR, EVAL_DIR, k=K_FOR_MAP
            )

            st.success(f"run íŒŒì¼ ì €ì¥ ì™„ë£Œ: {run_path} (rows={len(df_run)})")
            if map_value is not None:
                st.info(f"MAP@{K_FOR_MAP} = {map_value:.4f}  (ë¦¬í¬íŠ¸: {ap_report_path})")
            else:
                st.warning("MAP ê³„ì‚°ì„ ìœ„í•œ run íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# ======================= Detail View =======================
def render_detail():
    cotid = st.session_state.get("selected_cotid")
    rec = next((r for r in ALL_RECORDS if str(r.get("cotId")) == str(cotid)), None)
    if not rec:
        st.warning("ìƒì„¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if st.button("â¬…ï¸ ë©”ì¸ìœ¼ë¡œ"):
            go_main()
            st.rerun()
        return

    st.title(f"ğŸ“ {rec.get('title', 'ìƒì„¸ì •ë³´')}")
    st.markdown(f"**ì§€ì—­:** {rec.get('area', '-')}  ")
    st.markdown(f"**ë“±ë¡ì¼:** {rec.get('create_date', '-')} / **ìˆ˜ì •ì¼:** {rec.get('update_date', '-')}")

    if rec.get("detail_url"):
        st.markdown(f"[ìƒì„¸ í˜ì´ì§€ ë°”ë¡œê°€ê¸°]({rec['detail_url']})")

    if st.button("â¬…ï¸ ë©”ì¸ìœ¼ë¡œ"):
        go_main()
        st.rerun()

    st.markdown("---")
    st.subheader("ê¸°ë³¸ ì •ë³´")
    basic = rec.get("basic_info") or {}
    if basic:
        for k, v in basic.items():
            st.write(f"- **{k}**: {v}")
    else:
        st.info("ê¸°ë³¸ ì •ë³´ ì—†ìŒ")

    st.subheader("ë¬´ì¥ì•  í¸ì˜ì •ë³´")
    bf = rec.get("barrierfree_info") or {}
    if bf and isinstance(bf, dict):
        for cat, kv in bf.items():
            st.write(f"### {cat}")
            if isinstance(kv, dict):
                for k, v in kv.items():
                    st.write(f"- {k}: {v}")
            else:
                st.write(kv)
    else:
        st.info("ë¬´ì¥ì•  í¸ì˜ì •ë³´ ì—†ìŒ")

    imgs = rec.get("images") or []
    if imgs:
        st.subheader("ì´ë¯¸ì§€")
        for i in range(0, len(imgs), 3):
            cols = st.columns(3)
            for j, url in enumerate(imgs[i:i+3]):
                with cols[j]:
                    st.image(url, use_container_width=True)

    if rec.get("lat") and rec.get("lng"):
        st.subheader("ì§€ë„")
        map_df = pd.DataFrame([{"latitude": rec["lat"], "longitude": rec["lng"]}])
        st.map(map_df)

# ======================= ì‹¤í–‰ =======================
if st.session_state["view"] == "detail":
    render_detail()
else:
    render_main()
